{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title # <font face='JetBrains Mono' color='#FA7921'>ðŸ”¶STEP-1ðŸ”¶</font>\n",
    "\n",
    "#@markdown **<font face='JetBrains Mono' color='#FA7921'>Install Dependencies and perform setup operations:**</font>\n",
    "\n",
    "#@markdown âž– <font face='JetBrains Mono' color='#FAF0CA'>Since some dependencies must be compiled from source code, it may take a while to complete.\n",
    "\n",
    "!git clone -b Optimization https://github.com/pawansharmaaaa/Lip_Wise\n",
    "\n",
    "%cd Lip_Wise\n",
    "\n",
    "!chmod +x ./setup-colab.sh\n",
    "!chmod +x ./launch.sh\n",
    "\n",
    "!clear\n",
    "!./setup-colab.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ##### **<font face='JetBrains Mono' color='#FA7921'>âš¡ Launch Lip_Wise:</font>**\n",
    "\n",
    "# @markdown âž– <font face='JetBrains Mono' color='#FAF0CA'>A live link will be generated in this step, which can be used to access the Lip_Wise's interface, It can be accessed from any device and it will remain accessible as long as this cell is running.</font>\n",
    "\n",
    "# @markdown â­• <font face='JetBrains Mono' color='#ED1C24'>*Sometimes Gradio Live doesn't work, In those times use the below cell instead.*</font>\n",
    "\n",
    "!./launch.sh --colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the folder you want to zip up\n",
    "folder_path = '/content/Lip_Wise/data'\n",
    "\n",
    "# Create a ZipFile object\n",
    "with zipfile.ZipFile('data.zip', 'w') as zip_ref:\n",
    "    # Walk through the folder and add each file to the zip\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            zip_ref.write(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"always\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is a part of https://github.com/pawansharmaaaa/Lip_Wise/ repository.\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import subprocess\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Custom Modules\n",
    "from helpers import audio\n",
    "from helpers import file_check\n",
    "from helpers import preprocess_mp as pmp\n",
    "from helpers import model_loaders\n",
    "from helpers import batch_processors\n",
    "\n",
    "# Global Variables\n",
    "TEMP_DIRECTORY = file_check.TEMP_DIR\n",
    "MEDIA_DIRECTORY = file_check.MEDIA_DIR\n",
    "NPY_FILES_DIRECTORY = file_check.NPY_FILES_DIR\n",
    "OUTPUT_DIRECTORY = file_check.OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r\"E:\\Lip_Wise_GFPGAN\\_testData\\Inputs\\testing_aud.mp4\"\n",
    "audio_path = r\"E:\\Lip_Wise_GFPGAN\\_testData\\Inputs\\testing_aud.mp3\"\n",
    "pad = 0\n",
    "align_3d = False\n",
    "face_restorer = 'CodeFormer'\n",
    "fps = 30\n",
    "mel_step_size = 16\n",
    "weight = 1.0\n",
    "upscale_bg = False\n",
    "bgupscaler = 'RealESRGAN_x4plus'\n",
    "gan = False\n",
    "loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform checks to ensure that all required files are present\n",
    "file_check.perform_check(bg_model_name=bgupscaler,\n",
    "                         restorer=face_restorer, use_gan_version=gan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIDEO INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input type\n",
    "input_type, vid_ext = file_check.get_file_type(video_path)\n",
    "if input_type != \"video\":\n",
    "    raise Exception(\"Input file is not a video. Try again with an video file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file is not a wav file. Converting to wav...\n"
     ]
    }
   ],
   "source": [
    "# Get audio type\n",
    "audio_type, aud_ext = file_check.get_file_type(audio_path)\n",
    "if audio_type != \"audio\":\n",
    "    raise Exception(\"Input file is not an audio.\")\n",
    "if aud_ext != \"wav\":\n",
    "    print(\"Audio file is not a wav file. Converting to wav...\")\n",
    "    # Convert audio to wav\n",
    "    command = 'ffmpeg -y -i {} -strict -2 {}'.format(\n",
    "        audio_path, os.path.join(MEDIA_DIRECTORY, 'aud_input.wav'))\n",
    "    subprocess.call(command, shell=True)\n",
    "    audio_path = os.path.join(MEDIA_DIRECTORY, 'aud_input.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Free Memory: 2.12 GB\n",
      "Limiting number of threads to 1.0 to avoid vram issues.\n",
      "Using cuda for inference.\n"
     ]
    }
   ],
   "source": [
    "# Check for cuda\n",
    "free_memory = torch.cuda.mem_get_info()[0]\n",
    "print(f\"Initial Free Memory: {free_memory/1024**3:.2f} GB\")\n",
    "\n",
    "# Limiting the number of threads to avoid vram issues\n",
    "limit = free_memory // 2e9\n",
    "print(f'Limiting number of threads to {limit} to avoid vram issues.')\n",
    "\n",
    "# Do not use GPU if free memory is less than 2GB\n",
    "device = 'cuda' if torch.cuda.is_available() and limit != 0 else 'cpu'\n",
    "print(f'Using {device} for inference.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create media_preprocess object and helper object\n",
    "processor = pmp.ModelProcessor(padding=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loop:\n",
    "    print(\"Looping video...\")\n",
    "    video_path = processor.loop_video(\n",
    "        video_path=video_path, audio_path=audio_path)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting face landmarks...\n"
     ]
    }
   ],
   "source": [
    "# Get face landmarks\n",
    "gr.Info(\"Getting face landmarks...\")\n",
    "processor.detect_for_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating audio spectrogram...\n",
      "Length of mel chunks: 895\n"
     ]
    }
   ],
   "source": [
    "# Generate audio spectrogram\n",
    "print(\"Generating audio spectrogram...\")\n",
    "wav = audio.load_wav(audio_path, 16000)\n",
    "mel = audio.melspectrogram(wav)\n",
    "\n",
    "mel_chunks = []\n",
    "# The mel_idx_multiplier aligns audio chunks with video frames for consistent processing and analysis.\n",
    "mel_idx_multiplier = 80./fps\n",
    "i = 0\n",
    "while 1:\n",
    "    start_idx = int(i * mel_idx_multiplier)\n",
    "    if start_idx + mel_step_size > len(mel[0]):\n",
    "        mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n",
    "        break\n",
    "    mel_chunks.append(mel[:, start_idx: start_idx + mel_step_size])\n",
    "    i += 1\n",
    "\n",
    "print(f\"Length of mel chunks: {len(mel_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video is longer than audio. Truncating video...\n"
     ]
    }
   ],
   "source": [
    "if len(mel_chunks) > frame_count:\n",
    "    print(\"Audio is longer than video. Truncating audio...\")\n",
    "    mel_chunks = mel_chunks[:frame_count]\n",
    "elif len(mel_chunks) < frame_count:\n",
    "    print(\"Video is longer than audio. Truncating video...\")\n",
    "    frame_count = len(mel_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "indices = random.sample(range(len(mel_chunks)), 16)\n",
    "indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `mels` is your list of mel-spectrograms\n",
    "mels = [mel_chunks[i] for i in indices]\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(15, 15))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    # Get the mel-spectrogram from the list\n",
    "    S = mels[i]\n",
    "\n",
    "    # Convert to log scale (dB)\n",
    "    D = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Plot the mel-spectrogram\n",
    "    img = librosa.display.specshow(D, x_axis='time', y_axis='mel', ax=ax)\n",
    "\n",
    "    # Remove the axis labels for clarity\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add a colorbar to the figure\n",
    "fig.colorbar(img, ax=axs, format=\"%+2.f dB\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Boolean mask\n",
    "total_frames = np.arange(0, frame_count)\n",
    "no_face_index = np.load(os.path.join(NPY_FILES_DIRECTORY, 'no_face_index.npy'))\n",
    "\n",
    "mask = np.isin(total_frames, no_face_index, invert=True).astype(bool)\n",
    "mask_batch = np.array_split(mask, len(mask)//16, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split mel chunks into batches\n",
    "mel_chunks = np.array(mel_chunks)\n",
    "mel_chunks_batch = np.array_split(mel_chunks, len(mel_chunks)//16, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of frame numbers and split it into batches\n",
    "frame_numbers = np.arange(0, frame_count)\n",
    "frame_nos_batch = np.array_split(frame_numbers, frame_count//16, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch helper object\n",
    "bp = batch_processors.BatchProcessors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load CodeFormer checkpoint from: e:\\Lip_Wise\\weights\\codeformers\\codeformer.pth\n",
      "Loading wav2lip checkpoint from: e:\\Lip_Wise\\weights\\wav2lip\\wav2lip.pth\n"
     ]
    }
   ],
   "source": [
    "# Create model loader object\n",
    "ml = model_loaders.ModelLoader(face_restorer, weight)\n",
    "\n",
    "# Load wav2lip model\n",
    "w2l_model = ml.load_wav2lip_model(gan=gan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Frames and rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory inside the data directory\n",
    "data_directory = \"data\"\n",
    "list_directory = os.path.join(data_directory, \"lists\")\n",
    "os.makedirs(list_directory, exist_ok=True)\n",
    "\n",
    "# Create directories with the name of each list\n",
    "lists = [\"images_list\", \"mels_list\", \"extracted_faces_list\", \"face_masks_list\", \"inv_masks_list\", \"cropped_faces_list\", \"frame_batch_list\", \"mel_batch_list\",\n",
    "         \"dubbed_faces_list\", \"restored_faces_list\", \"resized_restored_faces_list\", \"pasted_ready_faces_list\", \"ready_to_paste_list\", \"restored_images_list\", \"upscaled_bg_list\"]\n",
    "\n",
    "for list_name in lists:\n",
    "    list_path = os.path.join(list_directory, list_name)\n",
    "    os.makedirs(list_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_no = 0\n",
    "images = []\n",
    "while (batch_no < 16):\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_no = int(video.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    images.append(frame)\n",
    "\n",
    "    if len(images) == len(mask_batch[batch_no]):\n",
    "\n",
    "        frames = np.array(images)\n",
    "        frame_nos_to_input = frame_nos_batch[batch_no][mask_batch[batch_no]]\n",
    "        mels_to_input = mel_chunks_batch[batch_no][mask_batch[batch_no]]\n",
    "        frames_to_input = frames[mask_batch[batch_no]]\n",
    "\n",
    "        if len(frames_to_input) != 0 and len(mels_to_input) != 0:\n",
    "            extracted_faces, face_masks, inv_masks, centers, bboxes = bp.extract_face_batch(\n",
    "                frames_to_input, frame_nos_to_input)\n",
    "\n",
    "            # Save Data\n",
    "            extracted_faces_list.append(extracted_faces[0])\n",
    "            face_masks_list.append(face_masks[0])\n",
    "            inv_masks_list.append(inv_masks[0])\n",
    "\n",
    "            cropped_faces, aligned_bboxes, rotation_matrices = bp.align_crop_batch(\n",
    "                extracted_faces, frame_nos_to_input)\n",
    "\n",
    "            # Save Data\n",
    "            cropped_faces_list.append(cropped_faces[0])\n",
    "\n",
    "            frame_batch, mel_batch = bp.gen_data_video_mode(\n",
    "                cropped_faces, mels_to_input)\n",
    "            # Save Data\n",
    "            frame_batch_list.append(frame_batch[0])\n",
    "            mel_batch_list.append(mel_batch[0])\n",
    "\n",
    "            # Feed to wav2lip model:\n",
    "            frame_batch = torch.FloatTensor(\n",
    "                np.transpose(frame_batch, (0, 3, 1, 2))).to(device)\n",
    "            mel_batch = torch.FloatTensor(\n",
    "                np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dubbed_faces = w2l_model(mel_batch, frame_batch)\n",
    "\n",
    "            dubbed_faces = dubbed_faces.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
    "            # Save Data\n",
    "            dubbed_faces_list.append(dubbed_faces[0])\n",
    "\n",
    "            if face_restorer == 'CodeFormer':\n",
    "                with ThreadPoolExecutor(max_workers=limit) as executor:\n",
    "                    restored_faces = list(executor.map(\n",
    "                        ml.restore_wCodeFormer, dubbed_faces))\n",
    "            elif face_restorer == 'GFPGAN':\n",
    "                with ThreadPoolExecutor(max_workers=limit) as executor:\n",
    "                    restored_faces = list(executor.map(\n",
    "                        ml.restore_wGFPGAN, dubbed_faces))\n",
    "            elif face_restorer == 'RestoreFormer':\n",
    "                with ThreadPoolExecutor(max_workers=limit) as executor:\n",
    "                    restored_faces = list(executor.map(\n",
    "                        ml.restore_wRF, dubbed_faces))\n",
    "            elif face_restorer == \"None\":\n",
    "                restored_faces = dubbed_faces\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"Invalid face restorer model. Please check the model name and try again.\")\n",
    "\n",
    "            # Save Data\n",
    "            restored_faces_list.append(restored_faces[0])\n",
    "\n",
    "            # Post processing\n",
    "            resized_restored_faces = bp.face_resize_batch(\n",
    "                restored_faces, cropped_faces)\n",
    "            resized_restored_faces_list.append(resized_restored_faces[0])\n",
    "            pasted_ready_faces = bp.paste_back_black_bg_batch(\n",
    "                resized_restored_faces, aligned_bboxes, frames_to_input, ml)\n",
    "            pasted_ready_faces_list.append(pasted_ready_faces[0])\n",
    "            ready_to_paste = bp.unwarp_align_batch(\n",
    "                pasted_ready_faces, rotation_matrices)\n",
    "            ready_to_paste_list.append(ready_to_paste[0])\n",
    "            restored_images = bp.paste_back_batch(\n",
    "                ready_to_paste, frames_to_input, face_masks, inv_masks, centers)\n",
    "            restored_images_list.append(restored_images[0])\n",
    "\n",
    "            if upscale_bg:\n",
    "                frame, _ = ml.restore_background(\n",
    "                    restored_images[0], bgupscaler, tile=400, outscale=1.0, half=False)\n",
    "                upscaled_bg_list.append(frame)\n",
    "\n",
    "        print(f\"Writing batch no: {batch_no+1} out of total 16 batches.\")\n",
    "        batch_no += 1\n",
    "\n",
    "        images = []\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = np.asarray(images_list)\n",
    "mels_list = np.asarray(mels_list)\n",
    "extracted_faces_list = np.asarray(extracted_faces_list)\n",
    "face_masks_list = np.asarray(face_masks_list)\n",
    "inv_masks_list = np.asarray(inv_masks_list)\n",
    "cropped_faces_list = np.asarray(cropped_faces_list)\n",
    "frame_batch_list = np.asarray(frame_batch_list)\n",
    "mel_batch_list = np.asarray(mel_batch_list)\n",
    "dubbed_faces_list = np.asarray(dubbed_faces_list)\n",
    "restored_faces_list = np.asarray(restored_faces_list)\n",
    "resized_restored_faces_list = np.asarray(resized_restored_faces_list)\n",
    "pasted_ready_faces_list = np.asarray(pasted_ready_faces_list)\n",
    "ready_to_paste_list = np.asarray(ready_to_paste_list)\n",
    "restored_images_list = np.asarray(restored_images_list)\n",
    "upscaled_bg_list = np.asarray(upscaled_bg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `images` and `mels` are your lists of images and mel-spectrograms as numpy arrays\n",
    "images_rand = \n",
    "mels_rand = \n",
    "\n",
    "fig, axs = plt.subplots(len(images_rand), 3, figsize=(15, 50))\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    # Split the concatenated image into two separate images\n",
    "    img1 = images_rand[i][:, :, :3]\n",
    "    img2 = images_rand[i][:, :, 3:]\n",
    "\n",
    "    # Plot the images and mel-spectrogram\n",
    "    ax[0].imshow(img1[:,:,::-1])\n",
    "    ax[1].imshow(img2[:,:,::-1])\n",
    "\n",
    "    # Convert to log scale (dB). If the mel-spectrogram is already in dB, you can skip this.\n",
    "    D = librosa.power_to_db(mels_rand[i], ref=np.max)\n",
    "    # Squeeze the last dimension of D\n",
    "    D = np.squeeze(D)\n",
    "    # Plot the mel-spectrogram\n",
    "    img = librosa.display.specshow(D, x_axis='time', y_axis='mel', ax=ax[2])\n",
    "\n",
    "    # Remove the axis labels for clarity\n",
    "    ax[0].axis('off')\n",
    "    ax[1].axis('off')\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    # Add labels\n",
    "    if i == 0:\n",
    "        ax[0].set_title('Image')\n",
    "        ax[1].set_title('Masked')\n",
    "        ax[2].set_title('Mel Spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"ffmpeg -y -i {audio_path} -i {os.path.join(MEDIA_DIRECTORY, 'temp.mp4')} -strict -2 -q:v 1 {os.path.join(OUTPUT_DIRECTORY, file_name)}\"\n",
    "subprocess.call(command, shell=platform.system() != 'Windows')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lip-wise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
